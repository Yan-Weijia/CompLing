{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW03.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yan-Weijia/CompLing/blob/master/HW03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqOgS_2_lhVY",
        "colab_type": "text"
      },
      "source": [
        "预备工作：\n",
        "1. 在text的基础上建dict1  词:频次\n",
        "2. 遍历(обходим)词典里的每个词，为每一个建list  去掉n个字母的所有形式 （n=1）\n",
        "3. 每一个得到的形式作为key放到dict2，源词是value；不同value相同的key，value写成list；此处可能有重复key，解决方法是先set()？\n",
        "\n",
        "算法：\n",
        "1. 选出词，看它在不在dict1；如果在，直接返回该词\n",
        "2. 如果不在，建立去掉n个字母的形式的list\n",
        "3. 在dict2中找每一个上一步得到的形式；如果找到，把value填入list\n",
        "4. 从得到的list中找到最高频（dict1）；此处在问2中加入3-gram，先3-gram，再高频词\n",
        "5. 如果词/形式不在任何的字典中，那无法纠正"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMzkTaSelgeL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, re # 文件路径库；字符串匹配库\n",
        "from string import punctuation\n",
        "import numpy as np # 维度数组和矩阵，针对数组运算的函数\n",
        "import json # 数据交换格式 \"key\":value {}object无序 []array有序 value可以是\"\"括起来的string, number, true, false, null, object, array\n",
        "from collections import Counter\n",
        "from pprint import pprint # 格式化输出\n",
        "from nltk import sent_tokenize\n",
        "punctuation += \"«»—…“”\"\n",
        "punct = set(punctuation)\n",
        "\n",
        "from sklearn.metrics import classification_report, accuracy_score # 机器学习算法库.矩阵运算；分类，精确度"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CTiqhh37SRt",
        "colab_type": "code",
        "outputId": "64cbbad1-bb05-47ca-9953-21177fb1dc92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGeOUvuUnOsN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "outputId": "801ca7b6-34ce-40e1-bcfe-c9d98e82b148"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/sents_with_mistakes.txt\n",
        "!wget https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/correct_sents.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-22 19:49:56--  https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/sents_with_mistakes.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 123167 (120K) [text/plain]\n",
            "Saving to: ‘sents_with_mistakes.txt’\n",
            "\n",
            "\rsents_with_mistakes   0%[                    ]       0  --.-KB/s               \rsents_with_mistakes 100%[===================>] 120.28K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2019-11-22 19:49:57 (3.08 MB/s) - ‘sents_with_mistakes.txt’ saved [123167/123167]\n",
            "\n",
            "--2019-11-22 19:50:08--  https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/correct_sents.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 120672 (118K) [text/plain]\n",
            "Saving to: ‘correct_sents.txt’\n",
            "\n",
            "correct_sents.txt   100%[===================>] 117.84K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2019-11-22 19:50:08 (2.99 MB/s) - ‘correct_sents.txt’ saved [120672/120672]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUtpeAobnWCw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bad = open('sents_with_mistakes.txt', encoding='utf8').read().splitlines()\n",
        "true = open('correct_sents.txt', encoding='utf8').read().splitlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCe6_dW2rPus",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "outputId": "44bc0cf8-42c0-4fd3-ae35-ba87a338587c"
      },
      "source": [
        "!wget https://github.com/mannefedov/compling_nlp_hse_course/raw/master/data/corpus_500.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-22 19:53:19--  https://github.com/mannefedov/compling_nlp_hse_course/raw/master/data/corpus_500.txt\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/corpus_500.txt [following]\n",
            "--2019-11-22 19:53:20--  https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/corpus_500.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1181403 (1.1M) [text/plain]\n",
            "Saving to: ‘corpus_500.txt’\n",
            "\n",
            "corpus_500.txt      100%[===================>]   1.13M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2019-11-22 19:53:21 (12.9 MB/s) - ‘corpus_500.txt’ saved [1181403/1181403]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RL0GM9afjdOq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(text):\n",
        "    normalized_text = [(word.strip(punctuation)) for word in text.lower().split()] # 按空格拆成单词后去标点，此列表项为word\n",
        "    normalized_text = [word for word in normalized_text if word] # 去除''空项\n",
        "    return normalized_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tW9awhz1jdPR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = [] # [[\"\",\"\"],[\"\",\"\"],[\"\",\"\"]] 把词按句子分组\n",
        "for text in open('corpus_500.txt').read().splitlines():\n",
        "    sents = sent_tokenize(text)\n",
        "    norm_sents = [normalize(sent) for sent in sents]\n",
        "    corpus += norm_sents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vftd8H_djdSM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "WORDS = Counter() # corpus词出现的频次\n",
        "for sent in corpus:\n",
        "    WORDS.update(sent)\n",
        "dict_corp = dict(WORDS) # 库词：频次"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_EPWXMLx_Np",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "9ea76a65-8f47-457b-d5df-ad85a93ade9f"
      },
      "source": [
        "WORDS.most_common(10)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('в', 4263),\n",
              " ('и', 1999),\n",
              " ('на', 1757),\n",
              " ('что', 1236),\n",
              " ('с', 990),\n",
              " ('по', 850),\n",
              " ('не', 739),\n",
              " ('он', 425),\n",
              " ('из', 406),\n",
              " ('этом', 405)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcnrYvtCjdP2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set去重corpus词\n",
        "vocab = set() # set会去重，列表update到set：列表中的项添加到set\n",
        "\n",
        "for sent in corpus:\n",
        "    vocab.update(sent)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YLJLKbbr0cnc"
      },
      "source": [
        "单个库词去掉n个字母的各种形式list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgPZb0p8jdU2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def edits1(word):\n",
        "    \"Создаем кандидатов, которые отличаются на одну букву \\\n",
        "    找出差一个字母的选项\"\n",
        "    letters    = 'йцукенгшщзхъфывапролджэячсмитьбюё'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R] # 漏打一个字母\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1 and R[0] != R[1]] # 颠倒两字母顺序\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters] # 错打成别的字母\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters] # 多打一个字母\n",
        "    return list(set(deletes + transposes + replaces + inserts))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITFBZhH_10bV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 建dict_corp2 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0rh-6Kuf1Ygs",
        "colab": {}
      },
      "source": [
        "def correction(word): \n",
        "    \"Находим наиболее вероятное похожее слово \\\n",
        "    相似词中最高概率的\"\n",
        "    return max(candidates(word), key=P)\n",
        "\n",
        "def candidates(word): \n",
        "    \"Генерируем кандидатов на исправление \\\n",
        "    产生用来对照修正的正确词\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words): \n",
        "    \"Выбираем слова, которые есть в корпусе \\\n",
        "    选出库中有的词\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    \"Создаем кандидатов, которые отличаются на одну букву \\\n",
        "    找出差一个字母的选项\"\n",
        "    letters    = 'йцукенгшщзхъфывапролджэячсмитьбюё'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R] # 漏打一个字母\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1] # 颠倒两字母顺序\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters] # 错打成别的字母\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters] # 多打一个字母\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word): \n",
        "    \"Создаем кандидатов, которые отличаются на две буквы \\\n",
        "    找出差两个字母的选项\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1)) # 多循环一遍edits1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SzDVwzQxGIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for w in dict_corp: \n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}