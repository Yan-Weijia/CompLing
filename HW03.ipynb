{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW03.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yan-Weijia/CompLing/blob/master/HW03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqOgS_2_lhVY",
        "colab_type": "text"
      },
      "source": [
        "# Задание 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMzkTaSelgeL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, re # 文件路径库；字符串匹配库\n",
        "from string import punctuation\n",
        "import numpy as np # 维度数组和矩阵，针对数组运算的函数\n",
        "import json # 数据交换格式 \"key\":value {}object无序 []array有序 value可以是\"\"括起来的string, number, true, false, null, object, array\n",
        "from collections import Counter\n",
        "from pprint import pprint # 格式化输出\n",
        "from nltk import sent_tokenize\n",
        "punctuation += \"«»—…“”\"\n",
        "punct = set(punctuation)\n",
        "\n",
        "from sklearn.metrics import classification_report, accuracy_score # 机器学习算法库.矩阵运算；分类，精确度"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CTiqhh37SRt",
        "colab_type": "code",
        "outputId": "9feebdad-62df-4ae5-d1d9-4c820315b2d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGeOUvuUnOsN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "outputId": "a01d3be7-3a2d-4878-f433-78ed96b0c9db"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/sents_with_mistakes.txt\n",
        "!wget https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/correct_sents.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-22 20:59:23--  https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/sents_with_mistakes.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 123167 (120K) [text/plain]\n",
            "Saving to: ‘sents_with_mistakes.txt’\n",
            "\n",
            "\rsents_with_mistakes   0%[                    ]       0  --.-KB/s               \rsents_with_mistakes 100%[===================>] 120.28K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2019-11-22 20:59:23 (3.07 MB/s) - ‘sents_with_mistakes.txt’ saved [123167/123167]\n",
            "\n",
            "--2019-11-22 20:59:25--  https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/correct_sents.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 120672 (118K) [text/plain]\n",
            "Saving to: ‘correct_sents.txt’\n",
            "\n",
            "correct_sents.txt   100%[===================>] 117.84K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2019-11-22 20:59:26 (2.97 MB/s) - ‘correct_sents.txt’ saved [120672/120672]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUtpeAobnWCw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bad = open('sents_with_mistakes.txt', encoding='utf8').read().splitlines()\n",
        "true = open('correct_sents.txt', encoding='utf8').read().splitlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCe6_dW2rPus",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "outputId": "37a7e0ec-4471-4cb9-ca24-29b9817438af"
      },
      "source": [
        "!wget https://github.com/mannefedov/compling_nlp_hse_course/raw/master/data/corpus_500.txt"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-22 20:59:35--  https://github.com/mannefedov/compling_nlp_hse_course/raw/master/data/corpus_500.txt\n",
            "Resolving github.com (github.com)... 192.30.253.113\n",
            "Connecting to github.com (github.com)|192.30.253.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/corpus_500.txt [following]\n",
            "--2019-11-22 20:59:36--  https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/corpus_500.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1181403 (1.1M) [text/plain]\n",
            "Saving to: ‘corpus_500.txt’\n",
            "\n",
            "corpus_500.txt      100%[===================>]   1.13M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2019-11-22 20:59:37 (14.5 MB/s) - ‘corpus_500.txt’ saved [1181403/1181403]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zo4-yiut9y9C",
        "colab": {}
      },
      "source": [
        "# txt里列表里每一句拆分成词 [[词，词，词，词]，[词，词，词]]\n",
        "def norm_txt(text):\n",
        "  new_txt = []\n",
        "  for sent in text:\n",
        "    tokens = sent.lower().split()\n",
        "    tokens = [re.sub('(^\\W+|\\W+$)', '', tkn) for tkn in tokens if (set(tkn)-punct)]\n",
        "    new_txt.append(tokens)\n",
        "  return new_txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqyFcsNz7zsV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "badlist = norm_txt(bad)\n",
        "truelist = norm_txt(true)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tW9awhz1jdPR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = [] # [[\"\",\"\"],[\"\",\"\"],[\"\",\"\"]] 把词按句子分组\n",
        "for text in open('corpus_500.txt').read().splitlines():\n",
        "    sents = sent_tokenize(text)\n",
        "    norm_sents = [normalize(sent) for sent in sents]\n",
        "    corpus += norm_sents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vftd8H_djdSM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "WORDS = Counter() # corpus词出现的频次\n",
        "for sent in corpus:\n",
        "    WORDS.update(sent)\n",
        "dict_corp = dict(WORDS) # 库词：频次"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_EPWXMLx_Np",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "262ef30d-c2ba-4691-8cb4-73a61e444651"
      },
      "source": [
        "WORDS.most_common(10)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('в', 4263),\n",
              " ('и', 1999),\n",
              " ('на', 1757),\n",
              " ('что', 1236),\n",
              " ('с', 990),\n",
              " ('по', 850),\n",
              " ('не', 739),\n",
              " ('он', 425),\n",
              " ('из', 406),\n",
              " ('этом', 405)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcnrYvtCjdP2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set去重corpus词\n",
        "vocab = set() # set会去重，列表update到set：列表中的项添加到set\n",
        "\n",
        "for sent in corpus:\n",
        "    vocab.update(sent)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YLJLKbbr0cnc"
      },
      "source": [
        "单个库词去掉n个字母的各种形式list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgPZb0p8jdU2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def edits(word):\n",
        "    \"Создаем кандидатов, которые отличаются на одну букву \\\n",
        "    找出差一个字母的选项\"\n",
        "    letters    = 'йцукенгшщзхъфывапролджэячсмитьбюё'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R] # 漏打一个字母\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1 and R[0] != R[1]] # 颠倒两字母顺序\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters] # 错打成别的字母\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters] # 多打一个字母\n",
        "    return list(set(deletes + transposes + replaces + inserts))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITFBZhH_10bV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 建dict_corp2 {错形式：[正确词], 错形式：[正确词, 正确词]}\n",
        "dict_corp2 = {}\n",
        "for word in dict_corp:\n",
        "  for edword in edits(word):\n",
        "    if edword not in dict_corp2:\n",
        "      dict_corp2[edword] = [word]\n",
        "    else:\n",
        "      dict_corp2[edword].append(word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTB5LkwbIvev",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cb939fcd-8433-40c9-d002-87252ae09d10"
      },
      "source": [
        "st = set()\n",
        "st.update([1,2])\n",
        "st.update([1,2,3,4])\n",
        "print(st)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{1, 2, 3, 4}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0rh-6Kuf1Ygs",
        "colab": {}
      },
      "source": [
        "def edits1(word):\n",
        "    \"Создаем кандидатов, которые отличаются на одну букву \\\n",
        "    找出差一个字母的选项\"\n",
        "    letters    = 'йцукенгшщзхъфывапролджэячсмитьбюё'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R] # 漏打一个字母的错误选项\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1] # 颠倒两字母顺序的错误选项\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters] # 改一个字母的错误选项\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters] # 加一个字母的错误选项\n",
        "    return set(deletes + transposes + replaces + inserts + word.split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "paaR0jPrds81",
        "colab": {}
      },
      "source": [
        "def correction(badword):\n",
        "  if badword in dict_corp:\n",
        "    corrected = badword # 没错不用改\n",
        "  else:\n",
        "    bad_forms = edits1(badword) # {bad1, bad2, ...}\n",
        "    variants = set()\n",
        "    for form in bad_forms:\n",
        "      if form not in dict_corp2:\n",
        "        corrected = form # 此错库中无，或正好改对了\n",
        "      else:\n",
        "        variants.update(dict_corp2[form])\n",
        "    dict_vars = {} # {候选：频次}\n",
        "    for var in list(variants):\n",
        "      dict_vars[var] = dict_corp[var]\n",
        "    for var in dict_vars.keys():\n",
        "      if dict_vars[var] == max(freq for freq in dict_vars.values()):\n",
        "        corrected = var\n",
        "  return corrected"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXAoQ91Xe_w3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "55d1b62c-d3e1-4dd3-c4d4-c24f494bbe0d"
      },
      "source": [
        "%%time\n",
        "print(correction('сонце'))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "конце\n",
            "CPU times: user 1.11 ms, sys: 1.02 ms, total: 2.13 ms\n",
            "Wall time: 1.69 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTbw4v76jda_",
        "colab_type": "code",
        "outputId": "8a6c31d0-7302-40e0-a784-60f1e3147f79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "%%time\n",
        "correction('солнце')"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 5 µs, sys: 1 µs, total: 6 µs\n",
            "Wall time: 7.87 µs\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'солнце'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmpfmMWPjdbR",
        "colab_type": "code",
        "outputId": "dd317b2c-30d9-458b-9828-f7ea1e397ba9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "%%time\n",
        "correction('насмехатьсяаававттававаываываы')"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4.62 ms, sys: 0 ns, total: 4.62 ms\n",
            "Wall time: 4.37 ms\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'насмехатьсяаававттававадываываы'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpVxB9JrjdFf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# напишем функцию, которая будет сопоставлять слова в правильном и ошибочном варианте\n",
        "# разобьем предложение по пробелам и удалим пунктуация на границах слов\n",
        "# 匹配正确和错误的对(alignment)；按空格把句子拆分成词，去除标点\n",
        "def align_words(sent_1, sent_2):\n",
        "    tokens_1 = sent_1.lower().split()\n",
        "    tokens_2 = sent_2.lower().split()\n",
        "    \n",
        "    tokens_1 = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_1 if (set(token)-punct)]\n",
        "    tokens_2 = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_2 if (set(token)-punct)]\n",
        "    \n",
        "    return list(zip(tokens_1, tokens_2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0vOlzPsjdZ6",
        "colab_type": "code",
        "outputId": "b10323da-d035-467d-e0f0-89256f5b8e0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "correct = 0 # 订正后对的总词数（本来对没动的 + 本来错改对的）\n",
        "total = 0 # 总词数\n",
        "\n",
        "total_mistaken = 0 # 总错词数\n",
        "mistaken_fixed = 0 # 改对了数\n",
        "\n",
        "total_correct = 0 # 本来是对的数\n",
        "correct_broken = 0 # 改错了数\n",
        "\n",
        "cashed = {}\n",
        "for i in range(len(true)): # 正确词的数量\n",
        "    word_pairs = align_words(true[i], bad[i]) # 对照配对\n",
        "    for pair in word_pairs:\n",
        "        total += 1 # 总词数\n",
        "        predicted = cashed.get(pair[1], correction(pair[1])) # 错误词、词库修正后词配对字典\n",
        "        cashed[pair[0]] = predicted # 正确词、词库修正后词\n",
        "        if predicted == pair[0]: # 修正词 == 对词\n",
        "            correct += 1 # 订正后对的总词数\n",
        "        \n",
        "        if pair[0] == pair[1]: # 对词 == 对错\n",
        "            total_correct += 1 # 错词是对的数\n",
        "            if pair[0] !=  predicted: # 对词 != 修正词\n",
        "                correct_broken += 1 # 改错了数\n",
        "        else: # 错词 != 对词\n",
        "            total_mistaken += 1 # 总错词数\n",
        "            if pair[0] == predicted: # 对词 == 修正词\n",
        "                mistaken_fixed += 1 # 改对了数\n",
        "        \n",
        "    if not i % 100:\n",
        "        print(i)\n",
        "        "
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yyr7knZjdaW",
        "colab_type": "code",
        "outputId": "bf44ed74-125f-4117-dc10-80ac6766e13d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(correct/total) # 订正后总对的数/总词数\n",
        "print(mistaken_fixed/total_mistaken) # 改对了数/总错词数\n",
        "print(correct_broken/total_correct) # 改错了数/本来是对的数"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4944055944055944\n",
            "0.1987720644666155\n",
            "0.4613529344205811\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6m7avOgrjdHz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mistakes = [] # 错误词list\n",
        "total = 0 # 总词数\n",
        "for i in range(len(true)): # len(true)是正确文本的句子数\n",
        "    word_pairs = align_words(true[i], bad[i]) # 每一句的正误词配对\n",
        "    for pair in word_pairs:\n",
        "        total += 1 # 每一句里的词总词数+1\n",
        "        if pair[0] != pair[1]: # 误词错误\n",
        "            mistakes.append(pair)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niK7UucpjdcX",
        "colab_type": "code",
        "outputId": "11b7a456-e3c7-478f-aaad-24c34d03fccc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "[(wt[0], wt[1], correction(wt[1])) for wt, _ in Counter(mistakes).most_common(10)] # mistakes: 错误词list"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('сегодня', 'седня', 'себя'),\n",
              " ('вообще', 'вобще', 'общей'),\n",
              " ('вообще', 'ваще', 'все'),\n",
              " ('естественно', 'естесственно', 'естественно'),\n",
              " ('хочется', 'хочеться', 'хочется'),\n",
              " ('кстати', 'кстате', 'стать'),\n",
              " ('очень', 'ооочень', 'очень'),\n",
              " ('как-то', 'както', 'как'),\n",
              " ('очень', 'оооочень', 'оьоочень'),\n",
              " ('это', 'ето', 'что')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKwOpUBXlyLm",
        "colab_type": "text"
      },
      "source": [
        "# Задание 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_aBFSsxql86",
        "colab_type": "code",
        "outputId": "b6e1a3d7-04bd-4edc-ddd7-35835614dfc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        }
      },
      "source": [
        "import itertools\n",
        "!pip install pymorphy2[fast]\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "from collections import Counter, defaultdict\n",
        "import numpy as np\n",
        "from string import punctuation\n",
        "from nltk.tokenize import sent_tokenize\n",
        "morph = MorphAnalyzer()"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pymorphy2[fast]\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 1.5MB/s \n",
            "\u001b[?25hCollecting pymorphy2-dicts<3.0,>=2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 7.2MB/s \n",
            "\u001b[?25hCollecting dawg-python>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2[fast]) (0.6.2)\n",
            "Collecting DAWG>=0.7.3; extra == \"fast\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/c0/d8d967bcaa0b572f9dc1d878bbf5a7bfd5afa2102a5ae426731f6ce3bc26/DAWG-0.7.8.tar.gz (255kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 47.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: DAWG\n",
            "  Building wheel for DAWG (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for DAWG: filename=DAWG-0.7.8-cp36-cp36m-linux_x86_64.whl size=764503 sha256=95933e5c362bb03feba956e61061f4f2e060e3a95722338b50fc84ac8695bbac\n",
            "  Stored in directory: /root/.cache/pip/wheels/d4/88/d0/4e4abc83eb8f59a71e8dbd8ba99fd5615a3af1fac1ef7f8825\n",
            "Successfully built DAWG\n",
            "Installing collected packages: pymorphy2-dicts, dawg-python, DAWG, pymorphy2\n",
            "Successfully installed DAWG-0.7.8 dawg-python-0.7.2 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfbVS-_UjdmR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences_corpus = [['<start>', '<start>'] + sent + ['<end>'] for sent in corpus]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l98msVj-3mfF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "b8771779-8ad6-4f87-fc67-ce657bb21387"
      },
      "source": [
        "print(sentences_corpus[:5])"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['<start>', '<start>', 'text', '<end>'], ['<start>', '<start>', 'вице-премьер', 'по', 'социальным', 'вопросам', 'татьяна', 'голикова', 'рассказала', 'в', 'каких', 'регионах', 'россии', 'зафиксирована', 'наиболее', 'высокая', 'смертность', 'от', 'рака', 'сообщает', 'риа', 'новости', '<end>'], ['<start>', '<start>', 'по', 'словам', 'голиковой', 'чаще', 'всего', 'онкологические', 'заболевания', 'становились', 'причиной', 'смерти', 'в', 'псковской', 'тверской', 'тульской', 'и', 'орловской', 'областях', 'а', 'также', 'в', 'севастополе', '<end>'], ['<start>', '<start>', 'вице-премьер', 'напомнила', 'что', 'главные', 'факторы', 'смертности', 'в', 'россии', 'рак', 'и', 'болезни', 'системы', 'кровообращения', '<end>'], ['<start>', '<start>', 'в', 'начале', 'года', 'стало', 'известно', 'что', 'смертность', 'от', 'онкологических', 'заболеваний', 'среди', 'россиян', 'снизилась', 'впервые', 'за', 'три', 'года', '<end>']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw_5wlZLrJNi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ngrammer(tokens, n):\n",
        "    ngrams = []\n",
        "    for i in range(0,len(tokens)-n+1):\n",
        "        ngrams.append(' '.join(tokens[i:i+n]))\n",
        "    return ngrams"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fC60y6XVu92R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unigrams = Counter()\n",
        "bigrams = Counter()\n",
        "trigrams = Counter()\n",
        "\n",
        "for sentence in sentences_corpus:\n",
        "  unigrams.update(sentence)\n",
        "  bigrams.update(ngrammer(sentence, 2))\n",
        "  trigrams.update(ngrammer(sentence, 3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4kzav3GVQYw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "matrix = np.zeros((len(bigrams), len(unigrams)))\n",
        "id2word_u = list(unigrams) # 列表值为('word', count)\n",
        "word2id_u = {word:i for i, word in enumerate(id2word_u)} # 字典值为('word', count):0（矩阵列定位）\n",
        "id2word_b = list(bigrams) # 列表值为('word', count)\n",
        "word2id_b = {word:i for i, word in enumerate(id2word_b)} # 字典值为('word', count):0（矩阵行定位）\n",
        "\n",
        "for ngram in trigrams:\n",
        "    word1, word2, word3 = ngram.split()\n",
        "    bigram = \"{} {}\".format(word1, word2)\n",
        "    matrix[word2id_b[bigram]][word2id_u[word3]] =  (trigrams[ngram]/bigrams[bigram])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "R4sJya1zjdnA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# оцените качество также как и раньше\n",
        "correct = 0 # 订正后对的总词数（本来对没动的 + 本来错改对的）\n",
        "total = 0 # 总词数\n",
        "\n",
        "mistakes = []\n",
        "total_mistaken = 0 # 总错词数\n",
        "mistaken_fixed = 0 # 改对了数\n",
        "\n",
        "total_correct = 0 # 本来是对的数\n",
        "correct_broken = 0 # 改错了数\n",
        "\n",
        "\n",
        "for i in range(len(true)):\n",
        "    word_pairs = align_words(true[i], bad[i])\n",
        "    \n",
        "    word_pairs = [('<start>', '<start>')] + word_pairs\n",
        "    pred_sent = []\n",
        "    for j in range(1, len(word_pairs)):\n",
        "        \n",
        "        pred = None\n",
        "        predicted = get_closest_hybrid_match(word_pairs[j][1], X, vec)\n",
        "        \n",
        "        \n",
        "        prev_word = word_pairs[j-1][1]\n",
        "        \n",
        "        \n",
        "        if prev_word not in unigrams:\n",
        "            pred = predicted[0][0]\n",
        "            \n",
        "        \n",
        "        else:\n",
        "            \n",
        "            lm_predicted = []\n",
        "            for word, m in predicted:\n",
        "                bigram = ' '.join([prev_word, word])\n",
        "                # домножаем полученную метрику для слова на вероятность биграма\n",
        "                # биграм - предыдущее слово + текущее слово кандидат\n",
        "                lm_predicted.append((word, (m)*(1+(bigrams[bigram]/unigrams[prev_word]))))\n",
        "            if lm_predicted:\n",
        "                \n",
        "                pred = sorted(lm_predicted, key=lambda x: -x[1])[0][0]\n",
        "            \n",
        "        \n",
        "        if pred is None:\n",
        "            pred = word_pairs[j][1]\n",
        "        \n",
        "\n",
        "        \n",
        "        if pred == word_pairs[j][0]:\n",
        "            correct += 1\n",
        "        else:\n",
        "            mistakes.append((word_pairs[j][0], word_pairs[j][1], pred))\n",
        "        total += 1\n",
        "            \n",
        "        if word_pairs[j][0] == word_pairs[j][1]:\n",
        "            total_correct += 1\n",
        "            if word_pairs[j][0] !=  pred:\n",
        "                correct_broken += 1\n",
        "        else:\n",
        "            total_mistaken += 1\n",
        "            if word_pairs[j][0] == pred:\n",
        "                mistaken_fixed += 1\n",
        "    \n",
        "    if not i % 50:\n",
        "        print(i)\n",
        "        print(correct/total)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqZ81PeWjdnN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(correct/total)\n",
        "print(mistaken_fixed/total_mistaken)\n",
        "print(correct_broken/total_correct)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}