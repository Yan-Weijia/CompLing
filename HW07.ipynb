{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Sense Disambiguation (снятие лексической неоднозначности) и Word Sense Induction (нахождение значений слова)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import adagram\n",
    "from lxml import html\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from string import punctuation\n",
    "import json, os\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "morph = MorphAnalyzer()\n",
    "punct = punctuation+'«»—…“”*№–'\n",
    "stops = set(stopwords.words('russian'))\n",
    "\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split() if word and word not in stops]\n",
    "    words = [word for word in words if word]\n",
    "\n",
    "    return words\n",
    "\n",
    "def normalize(text):\n",
    "    \n",
    "    words = tokenize(text)\n",
    "    words = [morph.parse(word)[0].normal_form for word in words if word]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание № 1. Протестировать адаграм в определении перефразирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm = adagram.VectorModel.load(\"out.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = vm.sense_vector(\"мир\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "corpus_xml = html.fromstring(open('paraphraser/paraphrases.xml', 'rb').read())\n",
    "texts_1 = []\n",
    "texts_2 = []\n",
    "classes = []\n",
    "\n",
    "for p in corpus_xml.xpath('//paraphrase'):\n",
    "    texts_1.append(p.xpath('./value[@name=\"text_1\"]/text()')[0])\n",
    "    texts_2.append(p.xpath('./value[@name=\"text_2\"]/text()')[0])\n",
    "    classes.append(p.xpath('./value[@name=\"class\"]/text()')[0])\n",
    "    \n",
    "data = pd.DataFrame({'text_1':texts_1, 'text_2':texts_2, 'label':classes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_1_norm'] = data['text_1'].apply(normalize)\n",
    "data['text_2_norm'] = data['text_2'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_1</th>\n",
       "      <th>text_2</th>\n",
       "      <th>label</th>\n",
       "      <th>text_1_norm</th>\n",
       "      <th>text_2_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Полицейским разрешат стрелять на поражение по ...</td>\n",
       "      <td>Полиции могут разрешить стрелять по хулиганам ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[полицейский, разрешить, стрелять, поражение, ...</td>\n",
       "      <td>[полиция, мочь, разрешить, стрелять, хулиган, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Право полицейских на проникновение в жилище ре...</td>\n",
       "      <td>Правила внесудебного проникновения полицейских...</td>\n",
       "      <td>0</td>\n",
       "      <td>[право, полицейский, проникновение, жилища, ре...</td>\n",
       "      <td>[правило, внесудебный, проникновение, полицейс...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Президент Египта ввел чрезвычайное положение в...</td>\n",
       "      <td>Власти Египта угрожают ввести в стране чрезвыч...</td>\n",
       "      <td>0</td>\n",
       "      <td>[президент, египет, ввести, чрезвычайный, поло...</td>\n",
       "      <td>[власть, египет, угрожать, ввести, страна, чре...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Вернувшихся из Сирии россиян волнует вопрос тр...</td>\n",
       "      <td>Самолеты МЧС вывезут россиян из разрушенной Си...</td>\n",
       "      <td>-1</td>\n",
       "      <td>[вернуться, сирия, россиянин, волновать, вопро...</td>\n",
       "      <td>[самолёт, мчс, вывезти, россиянин, разрушить, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>В Москву из Сирии вернулись 2 самолета МЧС с р...</td>\n",
       "      <td>Самолеты МЧС вывезут россиян из разрушенной Си...</td>\n",
       "      <td>0</td>\n",
       "      <td>[москва, сирия, вернуться, 2, самолёт, мчс, ро...</td>\n",
       "      <td>[самолёт, мчс, вывезти, россиянин, разрушить, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text_1  \\\n",
       "0  Полицейским разрешат стрелять на поражение по ...   \n",
       "1  Право полицейских на проникновение в жилище ре...   \n",
       "2  Президент Египта ввел чрезвычайное положение в...   \n",
       "3  Вернувшихся из Сирии россиян волнует вопрос тр...   \n",
       "4  В Москву из Сирии вернулись 2 самолета МЧС с р...   \n",
       "\n",
       "                                              text_2 label  \\\n",
       "0  Полиции могут разрешить стрелять по хулиганам ...     0   \n",
       "1  Правила внесудебного проникновения полицейских...     0   \n",
       "2  Власти Египта угрожают ввести в стране чрезвыч...     0   \n",
       "3  Самолеты МЧС вывезут россиян из разрушенной Си...    -1   \n",
       "4  Самолеты МЧС вывезут россиян из разрушенной Си...     0   \n",
       "\n",
       "                                         text_1_norm  \\\n",
       "0  [полицейский, разрешить, стрелять, поражение, ...   \n",
       "1  [право, полицейский, проникновение, жилища, ре...   \n",
       "2  [президент, египет, ввести, чрезвычайный, поло...   \n",
       "3  [вернуться, сирия, россиянин, волновать, вопро...   \n",
       "4  [москва, сирия, вернуться, 2, самолёт, мчс, ро...   \n",
       "\n",
       "                                         text_2_norm  \n",
       "0  [полиция, мочь, разрешить, стрелять, хулиган, ...  \n",
       "1  [правило, внесудебный, проникновение, полицейс...  \n",
       "2  [власть, египет, угрожать, ввести, страна, чре...  \n",
       "3  [самолёт, мчс, вывезти, россиянин, разрушить, ...  \n",
       "4  [самолёт, мчс, вывезти, россиянин, разрушить, ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Векторизуйте пары текстов с помощью Адаграма, обучите любую модель и оцените качество (кросс-валидацией). \n",
    "\n",
    "За основу возьмите код из предыдущего семинара/домашки, только в функции \n",
    "get_embedding вам нужно выбирать вектор нужного значения (импользуйте model.disambiguate и model.sense_vector). Отдельные векторы усредните как и в предыдущем семинаре."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для вытаскивания пар (целевое слово, контекстые слова) вам нужно будет написать специальную функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words = [0,1,2,3,4,5,6,7,8,9]\n",
    "def get_words_in_context(words, window=3):\n",
    "    words_context = []\n",
    "    for i, w in enumerate(words):\n",
    "        context = [words[c] for c in range(i - window, i + window + 1) if c != i and  c >= 0 and c < len(words)]  \n",
    "        words_context.append([w, context])\n",
    "    return words_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, [1, 2, 3]],\n",
       " [1, [0, 2, 3, 4]],\n",
       " [2, [0, 1, 3, 4, 5]],\n",
       " [3, [0, 1, 2, 4, 5, 6]],\n",
       " [4, [1, 2, 3, 5, 6, 7]],\n",
       " [5, [2, 3, 4, 6, 7, 8]],\n",
       " [6, [3, 4, 5, 7, 8, 9]],\n",
       " [7, [4, 5, 6, 8, 9]],\n",
       " [8, [5, 6, 7, 9]],\n",
       " [9, [6, 7, 8]]]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# работать должно вот так\n",
    "get_words_in_context(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда получиться такой же результат, добавьте эту функцию в get_embedding. Проходите циклом по результату работы get_words_in_context и поставляйте каждый элемент-список в model.disambiguate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_adagram(text, model, window, dim):\n",
    "    \n",
    "    word2context = get_words_in_context(text, window)\n",
    "    \n",
    "    vectors = np.zeros((len(word2context), dim))\n",
    "    \n",
    "    for i, (word, context) in enumerate(word2context):\n",
    "        \n",
    "        try:\n",
    "            index = model.disambiguate(word, context).argmax()\n",
    "            v = model.sense_vector(word, index)\n",
    "            vectors[i] = v\n",
    "        \n",
    "        except (KeyError, ValueError):\n",
    "            continue\n",
    "    \n",
    "    if vectors.any():\n",
    "        vector = np.average(vectors, axis=0)\n",
    "    else:\n",
    "        vector = np.zeros((dim))\n",
    "    \n",
    "    return vector\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 100\n",
    "X_text_1_vm = np.zeros((len(data['text_1_norm']), dim))\n",
    "X_text_2_vm = np.zeros((len(data['text_2_norm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    X_text_1_vm[i] = get_embedding_adagram(text, vm, 3, dim)\n",
    "    \n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    X_text_2_vm[i] = get_embedding_adagram(text, vm, 3, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_vm = np.concatenate([X_text_1_vm, X_text_2_vm], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7227, 200)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_text_vm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7227,)\n"
     ]
    }
   ],
   "source": [
    "y = data['label'].values\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.47      0.50      0.48       629\n",
      "           0       0.46      0.62      0.53       737\n",
      "           1       0.35      0.10      0.15       441\n",
      "\n",
      "    accuracy                           0.45      1807\n",
      "   macro avg       0.42      0.41      0.39      1807\n",
      "weighted avg       0.43      0.45      0.42      1807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_X, valid_X, train_y, valid_y = train_test_split(X_text_vm, y,random_state=1)\n",
    "clf = LogisticRegression(C=1000)\n",
    "clf.fit(train_X, train_y)\n",
    "preds = clf.predict(valid_X)\n",
    "print(classification_report(valid_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42046974858641645"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(clf, X_text_vm, y, scoring='f1_micro', cv=10).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2. Реализовать алгоритм Леска и проверить его на реальном датасете"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ворднет можно использовать для дизамбигуации. Самый простой алгоритм дизамбигуации - алгоритм Леска. В нём нужное значение слова находится через пересечение слов контекста, в котором употреблено это слово, с определениями значений слова из ворднета. Значение с максимальным пересечением - нужное."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# запустите если не установлен ворднет\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте его"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopw = set(stopwords.words('english'))\n",
    "def tok_en(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split() if word and word not in stopw]\n",
    "    words = [word for word in words if word]\n",
    "\n",
    "    return words\n",
    "\n",
    "def norm_en(text):\n",
    "    \n",
    "    words = tok_en(text)\n",
    "    words = [morph.parse(word)[0].normal_form for word in words if word]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lesk(word, sentence):\n",
    "    bestsense = 0\n",
    "    maxoverlap = 0\n",
    "    overlaps = []\n",
    "    \n",
    "    w_synsets = wn.synsets(word)\n",
    "    \n",
    "    # 所有的定义[[w, w, ...], [w, w, ...], ...]\n",
    "    defs = [synset.definition().split() for synset in wn.synsets(word)]\n",
    "    sent = [w.strip(punct) for w in sentence if w.strip(punct) and w not in stopw]\n",
    "    \n",
    "    for i, syns in enumerate(w_synsets):\n",
    "        defi = norm_en(syns.definition())\n",
    "        # defi = syns.definition().split()\n",
    "        # defi = [w.strip(punct) for w in defi if w.strip(punct) and w not in stopw]\n",
    "        overlap = len([i for i in sent if i in defi])\n",
    "        overlaps.append(overlap)\n",
    "    \n",
    "        maxoverlap = max(overlaps)\n",
    "        bestsense = overlaps.index(maxoverlap)\n",
    "        \n",
    "    return bestsense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работать функция должна как-то так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lesk('day', 'time for a planet to make a complete rotation on its axis'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the period of time taken by a particular planet (e.g. Mars) to make a complete rotation on its axis'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('day')[6].definition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проверьте насколько хорошо работает такой метод на реальном датасете.** http://lcl.uniroma1.it/wsdeval/ - большой фреймворк для оценки WSD. Там много данных и я взял кусочек, чтобы не было проблем с памятью"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_wsd = []\n",
    "corpus = open('corpus_wsd_50k.txt').read().split('\\n\\n')\n",
    "for sent in corpus:\n",
    "    corpus_wsd.append([s.split('\\t') for s in sent.split('\\n')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Корпус состоит из предложений, где у каждого слова три поля - значение, лемма и само слово. Значение пустое, когда слово однозначное, а у многозначных слов стоит тэг вида **'long%3:00:02::'** Это тэг wordnet'ного формата"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['', 'how', 'How'],\n",
       "  ['long%3:00:02::', 'long', 'long'],\n",
       "  ['', 'have', 'has'],\n",
       "  ['', 'it', 'it'],\n",
       "  ['be%2:42:03::', 'be', 'been'],\n",
       "  ['', 'since', 'since'],\n",
       "  ['', 'you', 'you'],\n",
       "  ['review%2:31:00::', 'review', 'reviewed'],\n",
       "  ['', 'the', 'the'],\n",
       "  ['objective%1:09:00::', 'objective', 'objectives'],\n",
       "  ['', 'of', 'of'],\n",
       "  ['', 'you', 'your'],\n",
       "  ['benefit%1:21:00::', 'benefit', 'benefit'],\n",
       "  ['', 'and', 'and'],\n",
       "  ['service%1:04:07::', 'service', 'service'],\n",
       "  ['program%1:09:01::', 'program', 'program'],\n",
       "  ['', '?', '?']],\n",
       " [['', 'have', 'Have'],\n",
       "  ['', 'you', 'you'],\n",
       "  ['permit%2:41:00::', 'permit', 'permitted'],\n",
       "  ['', 'it', 'it'],\n",
       "  ['', 'to', 'to'],\n",
       "  ['become%2:42:01::', 'become', 'become'],\n",
       "  ['', 'a', 'a'],\n",
       "  ['giveaway%1:21:00::', 'giveaway', 'giveaway'],\n",
       "  ['program%1:09:01::', 'program', 'program'],\n",
       "  ['rather%4:02:02::', 'rather', 'rather'],\n",
       "  ['', 'than', 'than'],\n",
       "  ['', 'one', 'one'],\n",
       "  ['', 'that', 'that'],\n",
       "  ['have%2:42:00::', 'have', 'has'],\n",
       "  ['', 'the', 'the'],\n",
       "  ['goal%1:09:00::', 'goal', 'goal'],\n",
       "  ['', 'of', 'of'],\n",
       "  ['improved%3:00:00::', 'improved', 'improved'],\n",
       "  ['employee%1:18:00::', 'employee', 'employee'],\n",
       "  ['morale%1:26:00::', 'morale', 'morale'],\n",
       "  ['', 'and', 'and'],\n",
       "  ['', ',', ','],\n",
       "  ['consequently%4:02:00::', 'consequently', 'consequently'],\n",
       "  ['', ',', ','],\n",
       "  ['increased%3:00:00::', 'increased', 'increased'],\n",
       "  ['productivity%1:07:00::', 'productivity', 'productivity'],\n",
       "  ['', '?', '?']]]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_wsd[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вам нужно для каждого многозначного слова (т.е. у него есть тэг в первом поле) с помощью алгоритма Леска предсказать нужный синсет и сравнить с правильным. Посчитайте процент правильных предсказаний (accuracy).**\n",
    "\n",
    "Если считается слишком долго, возьмите поменьше предложений (например, только тысячу)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "counter_true = 0\n",
    "for sent in corpus_wsd[:5000]:\n",
    "    sentence = [w[1] for w in sent]\n",
    "    for word in sent:\n",
    "        if word[0]:\n",
    "            counter += 1\n",
    "            true_syn = wn.lemma_from_key(word[0]).synset()\n",
    "            true = wn.synsets(word[1]).index(true_syn)\n",
    "            pred = lesk(word[1], sentence)\n",
    "            if true == pred:\n",
    "                counter_true += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4332027947131859"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = counter_true / counter\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дополнительный балл"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если хотите заработать дополнительный балл, попробуйте улучшить алгоритм Леска любым способом (например, использовать расстояние редактирования вместо пересечения или даже вставить машинное обучение)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
